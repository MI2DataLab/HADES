{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ArrayObject' from 'PyPDF2.generic' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/krzyzinskim/MI2/policy-comparison/PolicyComparison/mair-policies-comparision/NECP_flow.ipynb Kom√≥rka 1\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/krzyzinskim/MI2/policy-comparison/PolicyComparison/mair-policies-comparision/NECP_flow.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mload_data\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m process_all_documents\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/krzyzinskim/MI2/policy-comparison/PolicyComparison/mair-policies-comparision/NECP_flow.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mload_data\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataframe, process_text\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/krzyzinskim/MI2/policy-comparison/PolicyComparison/mair-policies-comparision/NECP_flow.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mplots\u001b[39;00m \u001b[39mimport\u001b[39;00m plot_counter_lemmas\n",
      "File \u001b[0;32m~/MI2/policy-comparison/PolicyComparison/mair-policies-comparision/load_data/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mload_data\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataframe, process_lemmas, process_text, process_tokens, read_txt\n",
      "File \u001b[0;32m~/MI2/policy-comparison/PolicyComparison/mair-policies-comparision/load_data/load_data.py:7\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Phrases\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m _multiply_ngrams, process_lemmas, process_tokens\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_txt\u001b[39m(path: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m     11\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/MI2/policy-comparison/PolicyComparison/mair-policies-comparision/load_data/utils.py:7\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPyPDF2\u001b[39;00m \u001b[39mimport\u001b[39;00m PdfFileReader\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_tokens\u001b[39m(\n\u001b[1;32m     11\u001b[0m     doc: pd\u001b[39m.\u001b[39mSeries, nlp: spacy\u001b[39m.\u001b[39mlanguage\u001b[39m.\u001b[39mLanguage, stop_words: List[\u001b[39mstr\u001b[39m]\n\u001b[1;32m     12\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m     13\u001b[0m     spacy_text \u001b[39m=\u001b[39m nlp(doc)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/PyPDF2/__init__.py:10\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mPyPDF2 is a free and open-source pure-python PDF library capable of splitting,\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mmerging, cropping, and transforming the pages of PDF files. It can also add\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mYou can read the full docs at https://pypdf2.readthedocs.io/.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_encryption\u001b[39;00m \u001b[39mimport\u001b[39;00m PasswordType\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_merger\u001b[39;00m \u001b[39mimport\u001b[39;00m PdfFileMerger, PdfMerger\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_page\u001b[39;00m \u001b[39mimport\u001b[39;00m PageObject, Transformation\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/PyPDF2/_encryption.py:36\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPyPDF2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m logger_warning\n\u001b[1;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPyPDF2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39merrors\u001b[39;00m \u001b[39mimport\u001b[39;00m DependencyError\n\u001b[0;32m---> 36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPyPDF2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgeneric\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     37\u001b[0m     ArrayObject,\n\u001b[1;32m     38\u001b[0m     ByteStringObject,\n\u001b[1;32m     39\u001b[0m     DictionaryObject,\n\u001b[1;32m     40\u001b[0m     PdfObject,\n\u001b[1;32m     41\u001b[0m     StreamObject,\n\u001b[1;32m     42\u001b[0m     TextStringObject,\n\u001b[1;32m     43\u001b[0m     create_string_object,\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCryptBase\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mencrypt\u001b[39m(\u001b[39mself\u001b[39m, data: \u001b[39mbytes\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbytes\u001b[39m:  \u001b[39m# pragma: no cover\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ArrayObject' from 'PyPDF2.generic' (unknown location)"
     ]
    }
   ],
   "source": [
    "from load_data.utils import process_all_documents\n",
    "from load_data import load_dataframe, process_text\n",
    "from plots import plot_counter_lemmas\n",
    "from plots.topics import interactive_exploration, plot_topics, plot_similarities\n",
    "from topic_modeling.utils import check_coherence_for_topics_num, tsne_dim_reduction, umap_dim_reduction, _topics_df\n",
    "from topic_modeling.lda_model import find_best_model, find_best_topics_num\n",
    "from topic_modeling.topic_probs import (\n",
    "    get_similarities,\n",
    "    get_topic_probs,\n",
    "    calculate_distance_matrix,\n",
    "    calculate_linkage_matrix,\n",
    "    topic_probs_by_column_binded,\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "necp_processed = pd.read_csv('./necp_reports/necp_processed.csv', index_col = 0)\n",
    "necp_processed.drop(['start_page', 'end_page', 'start_text', 'end_text'], axis = 1, inplace = True)\n",
    "necp_processed.drop(necp_processed[necp_processed.isnull()[\"text\"]].index, axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"paragraph\", \"country\", \"text_path\", \"text\", \"tokens\", \"lemmas\"])\n",
    "df[\"country\"] = necp_processed[\"country\"]\n",
    "df[\"text\"] = necp_processed[\"text\"]\n",
    "df[\"paragraph\"] = [row[1][\"subsection\"] if row[1][\"subsection\"] in [\"Overview and Process for Establishing the Plan\", \"Impact Assessment of Planned Policies and Measures\"] else row[1][\"energy_union_dimension\"] for row in necp_processed.iterrows()]\n",
    "processed_df = process_text(df, spacy_model=\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = [\"Overview and Process for Establishing the Plan\",\n",
    "              \"Impact Assessment of Planned Policies and Measures\",\n",
    "              \"Decarbonisation\",\n",
    "              \"Energy efficiency\",\n",
    "              \"Energy security\",\n",
    "              \"Internal market\",\n",
    "              \"R&I and Competitiveness\"]\n",
    "common_words = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph: Overview and Process for Establishing the Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dict = {'paragraph':paragraphs[0]}\n",
    "plot_counter_lemmas(processed_df, filter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_lemmas = processed_df.loc[(processed_df[list(filter_dict)] == pd.Series(filter_dict)).all(axis=1)][\"lemmas\"]\n",
    "counter = Counter(filtered_lemmas.sum()).most_common(3)\n",
    "common_words[paragraphs[0]] = [word for word, cnt in counter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph: Impact Assessment of Planned Policies and Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dict = {'paragraph':paragraphs[1]}\n",
    "plot_counter_lemmas(processed_df, filter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_lemmas = processed_df.loc[(processed_df[list(filter_dict)] == pd.Series(filter_dict)).all(axis=1)][\"lemmas\"]\n",
    "counter = Counter(filtered_lemmas.sum()).most_common(4)\n",
    "common_words[paragraphs[1]] = [word for word, cnt in counter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph: Decarbonisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dict = {'paragraph':paragraphs[2]}\n",
    "plot_counter_lemmas(processed_df, filter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_lemmas = processed_df.loc[(processed_df[list(filter_dict)] == pd.Series(filter_dict)).all(axis=1)][\"lemmas\"]\n",
    "counter = Counter(filtered_lemmas.sum()).most_common(5)\n",
    "common_words[paragraphs[2]] = [word for word, cnt in counter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph: Energy efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dict = {'paragraph':paragraphs[3]}\n",
    "plot_counter_lemmas(processed_df, filter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_lemmas = processed_df.loc[(processed_df[list(filter_dict)] == pd.Series(filter_dict)).all(axis=1)][\"lemmas\"]\n",
    "counter = Counter(filtered_lemmas.sum()).most_common(7)\n",
    "common_words[paragraphs[3]] = [word for word, cnt in counter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph: Energy security"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dict = {'paragraph':paragraphs[4]}\n",
    "plot_counter_lemmas(processed_df, filter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_lemmas = processed_df.loc[(processed_df[list(filter_dict)] == pd.Series(filter_dict)).all(axis=1)][\"lemmas\"]\n",
    "counter = Counter(filtered_lemmas.sum()).most_common(7)\n",
    "common_words[paragraphs[4]] = [word for word, cnt in counter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph: Internal market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dict = {'paragraph':paragraphs[5]}\n",
    "plot_counter_lemmas(processed_df, filter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_lemmas = processed_df.loc[(processed_df[list(filter_dict)] == pd.Series(filter_dict)).all(axis=1)][\"lemmas\"]\n",
    "counter = Counter(filtered_lemmas.sum()).most_common(5)\n",
    "common_words[paragraphs[5]] = [word for word, cnt in counter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph: R&I and Competitiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dict = {'paragraph':paragraphs[6]}\n",
    "plot_counter_lemmas(processed_df, filter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_lemmas = processed_df.loc[(processed_df[list(filter_dict)] == pd.Series(filter_dict)).all(axis=1)][\"lemmas\"]\n",
    "counter = Counter(filtered_lemmas.sum()).most_common(5)\n",
    "common_words[paragraphs[6]] = [word for word, cnt in counter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_numbers_range = (3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_pipeline(par, alpha):\n",
    "    print(f\"Pipeline for {par} with alpha={alpha} started\")\n",
    "    filter_dict = {'paragraph': par}\n",
    "    (filtered_lemmas, models, encoded_docs, lemmas_dictionary, cvs) = check_coherence_for_topics_num(\n",
    "        processed_df,\n",
    "        filter_dict,\n",
    "        common_words[par],\n",
    "        topic_numbers_range,\n",
    "        alpha\n",
    "    )\n",
    "    num_topics = find_best_topics_num(cvs, topic_numbers_range)\n",
    "    print(f\"Best number of topics found: {num_topics}\")\n",
    "    lda_model = find_best_model(encoded_docs, lemmas_dictionary, cvs, topic_numbers_range, random_state=42, alpha=alpha)\n",
    "    encoded_docs.to_csv(str(alpha) + \"_\" + par.replace(\" \", \"_\") +\"_encoded_docs.csv\")\n",
    "    lemmas_dictionary.save(str(alpha) + \"_\" + par.replace(\" \", \"_\") +\"_dictionary.dict\")\n",
    "    lda_model.save(str(alpha) + \"_\" + par.replace(\" \", \"_\") +\"_lda_model.model\")\n",
    "    print(\"Best model found and saved\")\n",
    "    topic_words = _topics_df(lda_model, filtered_lemmas, 30)\n",
    "    modeling_results, topic_probs = get_topic_probs(processed_df, filter_dict, lda_model, num_topics, encoded_docs)\n",
    "    topics_by_country = topic_probs_by_column_binded(modeling_results, num_topics, column='country')\n",
    "    result = tsne_dim_reduction(topics_by_country, num_topics * 3, perplexity=10)\n",
    "    result[[\"u1\", \"u2\"]] = umap_dim_reduction(result.iloc[:,:(num_topics * 3 + 1)], num_topics * 3, random_state=42)[[\"c1\", \"c2\"]]\n",
    "    result.to_csv(str(alpha) + \"_\" + par.replace(\" \", \"_\") +\"_probs.csv\")\n",
    "    topic_words.to_csv(str(alpha) + \"_\" + par.replace(\" \", \"_\") +\"_topic_words.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in [50, 100, 150, 200, 250]:\n",
    "    for par_idx in range(7):\n",
    "        do_pipeline(paragraphs[par_idx], alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from topic_modeling.topic_names import _generate_prompt, _generate_title\n",
    "import openai\n",
    "import os\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "gpt3_model = \"text-davinci-002\"\n",
    "temperature = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "result_files_list = glob.glob(\"../climate_results/*probs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result_file in tqdm(result_files_list[24:]): \n",
    "    topic_df = pd.read_csv(result_file)\n",
    "    topic_keywords = pd.read_csv(result_file.replace(\"probs\", \"topic_words\"))\n",
    "    colnames = topic_df.columns.to_list()\n",
    "    topic_colnames = colnames[1:-4]\n",
    "    one_section_flag = False\n",
    "    if \"Overview\" in result_file or \"Impact\" in result_file:\n",
    "        one_section_flag = True\n",
    "        n_topics = len(topic_colnames)\n",
    "    else:\n",
    "        n_topics = int(len(topic_colnames)/3)\n",
    "    for i, colname in enumerate(topic_colnames[:n_topics]):\n",
    "        time.sleep(1)\n",
    "        n_keywords = np.min([np.sum(topic_keywords[\"topic_id\"] == int(colname)), 20])\n",
    "        keywords = topic_keywords[topic_keywords[\"topic_id\"] == int(colname)].word.to_list()[:n_keywords]\n",
    "        weights = topic_keywords[topic_keywords[\"topic_id\"] == int(colname)].importance.to_list()[:n_keywords]\n",
    "        prompt = _generate_prompt(keywords, weights) \n",
    "        title = _generate_title(prompt, gpt3_model, temperature)\n",
    "        topic_colnames[i] = title\n",
    "    if not one_section_flag:\n",
    "        topic_colnames_final = []\n",
    "        for subsection in [\"NO&T \", \"P&M \", \"CS&RP \"]:\n",
    "            topic_colnames_final += [subsection + topic_name for topic_name in topic_colnames[:n_topics]]\n",
    "        topic_colnames = topic_colnames_final\n",
    "    colnames[1:-4] = topic_colnames\n",
    "    topic_df.columns = colnames\n",
    "    topic_df.to_csv(result_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import ast\n",
    "import pyLDAvis\n",
    "for result_file in tqdm(result_files_list): \n",
    "    lda_model = LdaModel.load(result_file.replace(\"_probs.csv\", \"lda_model.model\"))\n",
    "    encoded_docs = pd.read_csv(result_file.replace(\"_probs.csv\", \"_encoded_docs.csv\")).set_index(\"Unnamed: 0\")\n",
    "    encoded_docs.index.name = None\n",
    "    encoded_docs = encoded_docs.lemmas\n",
    "    encoded_docs = encoded_docs.apply(lambda x: ast.literal_eval(x))\n",
    "    lemmas_dictionary = Dictionary.load(result_file.replace(\"_probs.csv\", \"dictionary.dict\"))\n",
    "    vis = pyLDAvis.gensim_models.prepare(lda_model, encoded_docs, lemmas_dictionary)\n",
    "    vis_html_string = pyLDAvis.prepared_data_to_html(vis)\n",
    "    with open(result_file.replace(\"_probs.csv\", \"_vis.txt\"), \"w\") as text_file:\n",
    "        text_file.write(vis_html_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa6b73414a59142c661958c95b940f674ec289667fd8655973bfa9d81aed2c6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
