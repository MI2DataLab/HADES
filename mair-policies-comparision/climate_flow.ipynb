{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data.utils import process_all_documents\n",
    "from load_data import load_dataframe, process_text\n",
    "from plots import plot_counter_lemmas\n",
    "from plots.topics import interactive_exploration, plot_topics, plot_similarities\n",
    "from topic_modeling.utils import check_coherence_for_topics_num, tsne_dim_reduction, umap_dim_reduction, _topics_df\n",
    "from topic_modeling.lda_model import find_best_model, find_best_topics_num\n",
    "from topic_modeling.topic_probs import (\n",
    "    get_similarities,\n",
    "    get_topic_probs,\n",
    "    calculate_distance_matrix,\n",
    "    calculate_linkage_matrix,\n",
    "    topic_probs_by_column_binded,\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "necp_processed = pd.read_csv('./necp_reports/necp_processed.csv', index_col = 0)\n",
    "necp_processed.drop(['start_page', 'end_page', 'start_text', 'end_text'], axis = 1, inplace = True)\n",
    "necp_processed.drop(necp_processed[necp_processed.isnull()[\"text\"]].index, axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_stop_words = ['Austria', 'Austrian', 'Belgium', 'Belgian', 'Bulgaria', 'Bulgarian', 'Czech', 'Cyprus', 'Cypriot', 'Germany', 'German',\n",
    "                      'Denmark', 'Danish', 'Estonia', 'Estonian', 'Croatia', 'Croatian', 'Finland', 'Finnish', 'France', 'French', 'Malta', 'Maltese',\n",
    "                      'Luxembourg', 'Lithuania', 'Lithuanian', 'Latvia', 'Latvian', 'Italy', 'Italian', 'Ireland', 'Irish', 'Hungary', 'Hungarian',\n",
    "                      'Greece', 'Greek', 'Spain', 'Spanish', 'Netherlands', 'Dutch', 'Poland', 'Polish', 'Portugal', 'Portuguese', 'Romania', 'Romanian',\n",
    "                      'Sweden', 'Swedish', 'Slovenia', 'Slovenian', 'Slovakia', 'Slovak']\n",
    "extra_stop_words =  ['energy', 'figure', 'table', 'plan', \"necp\", 'national', 'use', \"measure\", \"sector\", \"climate\",\n",
    "                     \"plan\", \"dimension\", \"integrated\", \"section\", \"republic\", \"measures\", \"policies\", \"target\", \"objective\", \"policy\",\n",
    "                     \"projection\", \"assessment\", \"federal\", \"government\"]\n",
    "stop_words = [c.lower() for c in countries_stop_words]\n",
    "stop_words.extend(extra_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"paragraph\", \"country\", \"text_path\", \"text\", \"tokens\", \"lemmas\"])\n",
    "df[\"country\"] = necp_processed[\"country\"]\n",
    "df[\"text\"] = necp_processed[\"text\"]\n",
    "df[\"paragraph\"] = [row[1][\"subsection\"] if row[1][\"subsection\"] in [\"Overview and Process for Establishing the Plan\", \"Impact Assessment of Planned Policies and Measures\"] else row[1][\"energy_union_dimension\"] for row in necp_processed.iterrows()]\n",
    "processed_df = process_text(df, spacy_model=\"en_core_web_md\",  stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = [\"Overview and Process for Establishing the Plan\",\n",
    "              \"Impact Assessment of Planned Policies and Measures\",\n",
    "              \"Decarbonisation\",\n",
    "              \"Energy efficiency\",\n",
    "              \"Energy security\",\n",
    "              \"Internal market\",\n",
    "              \"R&I and Competitiveness\"]\n",
    "common_words = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph: Overview and Process for Establishing the Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dict = {'paragraph':paragraphs[0]}\n",
    "plot_counter_lemmas(processed_df, filter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_lemmas = processed_df.loc[(processed_df[list(filter_dict)] == pd.Series(filter_dict)).all(axis=1)][\"lemmas\"]\n",
    "counter = Counter(filtered_lemmas.sum()).most_common(1)\n",
    "common_words[paragraphs[0]] = [word for word, cnt in counter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph: Impact Assessment of Planned Policies and Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dict = {'paragraph':paragraphs[1]}\n",
    "plot_counter_lemmas(processed_df, filter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_lemmas = processed_df.loc[(processed_df[list(filter_dict)] == pd.Series(filter_dict)).all(axis=1)][\"lemmas\"]\n",
    "counter = Counter(filtered_lemmas.sum()).most_common(3)\n",
    "common_words[paragraphs[1]] = [word for word, cnt in counter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph: Decarbonisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dict = {'paragraph':paragraphs[2]}\n",
    "plot_counter_lemmas(processed_df, filter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_lemmas = processed_df.loc[(processed_df[list(filter_dict)] == pd.Series(filter_dict)).all(axis=1)][\"lemmas\"]\n",
    "counter = Counter(filtered_lemmas.sum()).most_common(5)\n",
    "common_words[paragraphs[2]] = [word for word, cnt in counter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph: Energy efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dict = {'paragraph':paragraphs[3]}\n",
    "plot_counter_lemmas(processed_df, filter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_lemmas = processed_df.loc[(processed_df[list(filter_dict)] == pd.Series(filter_dict)).all(axis=1)][\"lemmas\"]\n",
    "counter = Counter(filtered_lemmas.sum()).most_common(3)\n",
    "common_words[paragraphs[3]] = [word for word, cnt in counter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph: Energy security"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dict = {'paragraph':paragraphs[4]}\n",
    "plot_counter_lemmas(processed_df, filter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_lemmas = processed_df.loc[(processed_df[list(filter_dict)] == pd.Series(filter_dict)).all(axis=1)][\"lemmas\"]\n",
    "counter = Counter(filtered_lemmas.sum()).most_common(2)\n",
    "common_words[paragraphs[4]] = [word for word, cnt in counter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph: Internal market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dict = {'paragraph':paragraphs[5]}\n",
    "plot_counter_lemmas(processed_df, filter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_lemmas = processed_df.loc[(processed_df[list(filter_dict)] == pd.Series(filter_dict)).all(axis=1)][\"lemmas\"]\n",
    "counter = Counter(filtered_lemmas.sum()).most_common(4)\n",
    "common_words[paragraphs[5]] = [word for word, cnt in counter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph: R&I and Competitiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dict = {'paragraph':paragraphs[6]}\n",
    "plot_counter_lemmas(processed_df, filter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_lemmas = processed_df.loc[(processed_df[list(filter_dict)] == pd.Series(filter_dict)).all(axis=1)][\"lemmas\"]\n",
    "counter = Counter(filtered_lemmas.sum()).most_common(3)\n",
    "common_words[paragraphs[6]] = [word for word, cnt in counter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_numbers_range = (3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_pipeline(par, alpha):\n",
    "    print(f\"Pipeline for {par} with alpha={alpha} started\")\n",
    "    filter_dict = {'paragraph': par}\n",
    "    filter_dict = {'paragraph': par}\n",
    "    (filtered_lemmas, models, encoded_docs, lemmas_dictionary, cvs) = check_coherence_for_topics_num(\n",
    "        processed_df,\n",
    "        filter_dict,\n",
    "        common_words[par],\n",
    "        topic_numbers_range,\n",
    "        alpha\n",
    "    )\n",
    "    print(f\"Best number of topics found: {num_topics}\")\n",
    "    lda_model = find_best_model(encoded_docs, lemmas_dictionary, cvs, topic_numbers_range, random_state=42, alpha=alpha)\n",
    "    encoded_docs.to_csv(str(alpha) + \"_\" + par.replace(\" \", \"_\") +\"_encoded_docs.csv\")\n",
    "    lemmas_dictionary.save(str(alpha) + \"_\" + par.replace(\" \", \"_\") +\"_dictionary.dict\")\n",
    "    lda_model.save(str(alpha) + \"_\" + par.replace(\" \", \"_\") +\"_lda_model.model\")\n",
    "    print(\"Best model found and saved\")\n",
    "    topic_words = _topics_df(lda_model, filtered_lemmas, 30)\n",
    "    modeling_results, topic_probs = get_topic_probs(processed_df, filter_dict, lda_model, num_topics, encoded_docs)\n",
    "    topics_by_country = topic_probs_by_column_binded(modeling_results, num_topics, column='country')\n",
    "    topics_by_country.to_csv(str(alpha) + \"_\" + par.replace(\" \", \"_\") +\"_probs.csv\")\n",
    "    tsne_mapping = tsne_dim_reduction(topics_by_country, num_topics * 3, perplexity=10)\n",
    "    umap_mapping = umap_dim_reduction(topics_by_country, num_topics * 3, random_state=42)\n",
    "    mappings = tsne_mapping.join(umap_mapping)\n",
    "    mappings.to_csv(str(alpha) + \"_\" + par.replace(\" \", \"_\") +\"_mapping.csv\")\n",
    "    topic_words.to_csv(str(alpha) + \"_\" + par.replace(\" \", \"_\") +\"_topic_words.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in [50, 100, 150, 200, 250]:\n",
    "    for par_idx in range(7):\n",
    "        do_pipeline(paragraphs[par_idx], alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from topic_modeling.topic_names import _generate_prompt, _generate_title\n",
    "import openai\n",
    "import os\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "gpt3_model = \"text-davinci-002\"\n",
    "temperature = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "result_files_list = glob.glob(\"./climate_results/*probs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result_file in tqdm(result_files_list): \n",
    "    topic_df = pd.read_csv(result_file)\n",
    "    topic_keywords = pd.read_csv(result_file.replace(\"probs\", \"topic_words\"))\n",
    "    colnames = topic_df.columns.to_list()\n",
    "    topic_colnames = colnames[1:-4]\n",
    "    one_section_flag = False\n",
    "    if \"Overview\" in result_file or \"Impact\" in result_file:\n",
    "        one_section_flag = True\n",
    "        n_topics = len(topic_colnames)\n",
    "    else:\n",
    "        n_topics = int(len(topic_colnames)/3)\n",
    "    for i, colname in enumerate(topic_colnames[:n_topics]):\n",
    "        time.sleep(1)\n",
    "        n_keywords = np.min([np.sum(topic_keywords[\"topic_id\"] == int(colname)), 25])\n",
    "        keywords = topic_keywords[topic_keywords[\"topic_id\"] == int(colname)].word.to_list()[:n_keywords]\n",
    "        weights = topic_keywords[topic_keywords[\"topic_id\"] == int(colname)].importance.to_list()[:n_keywords]\n",
    "        prompt = _generate_prompt(keywords, weights) \n",
    "        title = _generate_title(prompt, gpt3_model, temperature)\n",
    "        topic_colnames[i] = title\n",
    "    if not one_section_flag:\n",
    "        topic_colnames_final = []\n",
    "        for subsection in [\"NO&T \", \"P&M \", \"CS&RP \"]:\n",
    "            topic_colnames_final += [subsection + topic_name for topic_name in topic_colnames[:n_topics]]\n",
    "        topic_colnames = topic_colnames_final\n",
    "    colnames[1:-4] = topic_colnames\n",
    "    topic_df.columns = colnames\n",
    "    topic_df.to_csv(result_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import ast\n",
    "import pyLDAvis\n",
    "for result_file in tqdm(result_files_list): \n",
    "    lda_model = LdaModel.load(result_file.replace(\"_probs.csv\", \"_lda_model.model\"))\n",
    "    encoded_docs = pd.read_csv(result_file.replace(\"_probs.csv\", \"_encoded_docs.csv\")).set_index(\"Unnamed: 0\")\n",
    "    encoded_docs.index.name = None\n",
    "    encoded_docs = encoded_docs.lemmas\n",
    "    encoded_docs = encoded_docs.apply(lambda x: ast.literal_eval(x))\n",
    "    lemmas_dictionary = Dictionary.load(result_file.replace(\"_probs.csv\", \"_dictionary.dict\"))\n",
    "    vis = pyLDAvis.gensim_models.prepare(lda_model, encoded_docs, lemmas_dictionary)\n",
    "    vis_html_string = pyLDAvis.prepared_data_to_html(vis)\n",
    "    with open(result_file.replace(\"_probs.csv\", \"_vis.txt\"), \"w\") as text_file:\n",
    "        text_file.write(vis_html_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
